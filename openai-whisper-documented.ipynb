{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e368b7",
   "metadata": {},
   "source": [
    "# Notebook Documentation: Audio Transcription and Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f3d9a",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates the process of transcribing audio files using OpenAI's Whisper model and summarizing the transcriptions. The notebook is divided into several sections, each performing specific tasks such as loading models, processing audio files, and summarizing the transcriptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a8fae",
   "metadata": {},
   "source": [
    "## Sections:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a4d6a2",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Environment Setup\n",
    "This section includes the necessary imports and environment configurations required for the notebook. It ensures that all dependencies are installed and the environment is ready for processing.\n",
    "```python\n",
    "import whisper\n",
    "import os\n",
    "from transformers import pipeline\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766069d6",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Load Whisper Model\n",
    "Here, the Whisper model from OpenAI is loaded. Whisper is an automatic speech recognition (ASR) system that can transcribe spoken language into text.\n",
    "```python\n",
    "model = whisper.load_model(\"base\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686aaa5",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Load and Transcribe Audio\n",
    "In this section, the audio file is loaded and transcribed into text using the Whisper model. The transcription is saved into a variable for further processing.\n",
    "```python\n",
    "# Load audio file\n",
    "audio = whisper.load_audio(\"path_to_audio_file\")\n",
    "# Transcribe audio\n",
    "result = model.transcribe(audio)\n",
    "transcription = result['text']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec0c24b",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Summarize Transcription\n",
    "The transcription obtained in the previous section is split into smaller chunks and summarized using a transformer-based summarization pipeline.\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Split transcription into chunks\n",
    "chunks = [transcription[i:i + 512] for i in range(0, len(transcription), 512)]\n",
    "\n",
    "# Summarize each chunk\n",
    "summaries = [summarizer(chunk, max_length=60, min_length=5)[0]['summary_text'] for chunk in chunks]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8bc46",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Combine Summaries\n",
    "The individual summaries are combined to form a coherent summary of the entire transcription.\n",
    "```python\n",
    "# Combine summaries\n",
    "combined_summary = \" \".join(summaries)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862b850",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Display Results\n",
    "Finally, the original transcription and the combined summary are displayed for comparison.\n",
    "```python\n",
    "print(\"Transcription:\", transcription)\n",
    "print(\"Summary:\", combined_summary)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-12T19:06:43.554498Z",
     "iopub.status.busy": "2024-04-12T19:06:43.553735Z",
     "iopub.status.idle": "2024-04-12T19:06:44.421999Z",
     "shell.execute_reply": "2024-04-12T19:06:44.421098Z",
     "shell.execute_reply.started": "2024-04-12T19:06:43.554450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/testing-audio/Y2meta.app-I Built a Personal Speech Recognition System for my AI Assistant-(480p).mp4\n",
      "/kaggle/input/testing-audio/videoplayback.weba\n",
      "/kaggle/input/audio1/ANIMAL_PEHLE BHI MAIN (Lyrical)  Ranbir KapoorTripti Dimri  Sandeep V  Vishal Mishra  Bhushan K.mp3\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:06:44.424596Z",
     "iopub.status.busy": "2024-04-12T19:06:44.424045Z",
     "iopub.status.idle": "2024-04-12T19:07:31.566561Z",
     "shell.execute_reply": "2024-04-12T19:07:31.565597Z",
     "shell.execute_reply.started": "2024-04-12T19:06:44.424563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.0\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: notebook in /opt/conda/lib/python3.10/site-packages (from jupyter) (6.5.4)\n",
      "Requirement already satisfied: qtconsole in /opt/conda/lib/python3.10/site-packages (from jupyter) (5.5.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/lib/python3.10/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.10/site-packages (from jupyter) (6.4.5)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from jupyter) (6.28.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from jupyter) (7.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (1.8.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (8.20.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (5.7.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (1.5.8)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (5.9.3)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (6.3.3)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->jupyter) (5.9.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter) (3.6.6)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->jupyter) (3.0.9)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter) (3.0.42)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from jupyter-console->jupyter) (2.17.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: jinja2>=2.4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: nbformat>=4.4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (5.9.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (0.4)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (6.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (0.5.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert->jupyter) (2.1.3)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter) (23.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter) (0.18.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter) (0.19.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook->jupyter) (1.0.0)\n",
      "Requirement already satisfied: qtpy>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from qtconsole->jupyter) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.0)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter) (2.13.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.3)\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter) (2.19.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.4->nbconvert->jupyter) (4.20.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/lib/python3.10/site-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert->jupyter) (0.16.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (4.2.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.9.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.5.1)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (7.4.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (2.4)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (2.8.19.20240106)\n",
      "Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Installing collected packages: jupyter\n",
      "Successfully installed jupyter-1.0.0\n",
      "Collecting pytube3\n",
      "  Downloading pytube3-9.6.4-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from pytube3) (4.9.0)\n",
      "Downloading pytube3-9.6.4-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: pytube3\n",
      "Successfully installed pytube3-9.6.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers jupyter \n",
    "!pip install pytube3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:07:31.568235Z",
     "iopub.status.busy": "2024-04-12T19:07:31.567926Z",
     "iopub.status.idle": "2024-04-12T19:07:48.451972Z",
     "shell.execute_reply": "2024-04-12T19:07:48.451226Z",
     "shell.execute_reply.started": "2024-04-12T19:07:31.568207Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 19:07:38.417619: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-12 19:07:38.417717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-12 19:07:38.548484: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:07:48.454482Z",
     "iopub.status.busy": "2024-04-12T19:07:48.453908Z",
     "iopub.status.idle": "2024-04-12T19:07:48.774772Z",
     "shell.execute_reply": "2024-04-12T19:07:48.773935Z",
     "shell.execute_reply.started": "2024-04-12T19:07:48.454454Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL of the audio file\n",
    "audio_url = \"https://www.uclass.psychol.ucl.ac.uk/Release2/Conversation/AudioOnly/wav/F_0101_10y4m_1.wav\"\n",
    "\n",
    "# Download the audio file\n",
    "response = requests.get(audio_url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"audio_file.wav\", \"wb\") as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:07:48.776076Z",
     "iopub.status.busy": "2024-04-12T19:07:48.775832Z",
     "iopub.status.idle": "2024-04-12T19:08:08.640819Z",
     "shell.execute_reply": "2024-04-12T19:08:08.639799Z",
     "shell.execute_reply.started": "2024-04-12T19:07:48.776056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff094c7a0cae40ceaad111fe3fc61e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62d57a3622c4511aad7e3dc6cc0f800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed809d8f3cac4968864b6203425a8eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a2495486ac4e1f9071f3d77e584ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b64baaa7144d6fbc972e7cddfe69eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dce1dd6f5e74edfb4c2da2d44cffc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e722cd41c84c9fbce6629289cd9574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40970b7d084e402ab4d8230cababb894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f9950858f141ab90a72db0ff599a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282225b95d044b1ab91ee453e0db00bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefa9e612ffb406da9ba55689c4ca9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the ASR pipeline\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:08.643021Z",
     "iopub.status.busy": "2024-04-12T19:08:08.642335Z",
     "iopub.status.idle": "2024-04-12T19:08:21.078713Z",
     "shell.execute_reply": "2024-04-12T19:08:21.077775Z",
     "shell.execute_reply.started": "2024-04-12T19:08:08.642985Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" What happened after she got out of the orphanage? She found the old building where she used to live. And she went inside it. Um, and she started to have a look around. Um... What does she find in the building? Um, these are pictures on the wall. and she found this old sort of like case thing. You've forgotten about the puppy. Where did you find the puppy? After she was thrown out, a puppy came running up. And who did she meet in the palace? A boy. Two men. What were they doing? Do you remember? No. They were trying to find, they were trying to audition, weren't they? People to pretend to be her. Yep. So they could get the money from the Grandmother. Great story. Yeah. and people to pretend to be her. Yep. So they could get the money from the grandma. It's a great story. Yeah. Where are we?\"}\n"
     ]
    }
   ],
   "source": [
    "transcription = pipe(\"audio_file.wav\")\n",
    "\n",
    "# Print the transcription\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:21.080900Z",
     "iopub.status.busy": "2024-04-12T19:08:21.080254Z",
     "iopub.status.idle": "2024-04-12T19:08:21.085210Z",
     "shell.execute_reply": "2024-04-12T19:08:21.084066Z",
     "shell.execute_reply.started": "2024-04-12T19:08:21.080864Z"
    }
   },
   "outputs": [],
   "source": [
    "# audio_file_path = \"/kaggle/input/audio1/ANIMAL_PEHLE BHI MAIN (Lyrical)  Ranbir KapoorTripti Dimri  Sandeep V  Vishal Mishra  Bhushan K.mp3\"\n",
    "\n",
    "# trans2 = pipe(audio_file_path)\n",
    "# print(trans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:21.086832Z",
     "iopub.status.busy": "2024-04-12T19:08:21.086555Z",
     "iopub.status.idle": "2024-04-12T19:08:33.021539Z",
     "shell.execute_reply": "2024-04-12T19:08:33.020566Z",
     "shell.execute_reply.started": "2024-04-12T19:08:21.086805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:33.023472Z",
     "iopub.status.busy": "2024-04-12T19:08:33.023127Z",
     "iopub.status.idle": "2024-04-12T19:08:42.393414Z",
     "shell.execute_reply": "2024-04-12T19:08:42.392356Z",
     "shell.execute_reply.started": "2024-04-12T19:08:33.023442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cb731d70ed44f0b139b1d962b04c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc1fc8d925642029b86f5624fe4270f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11efa5d4eb8841e3a31bb341563bc9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2705d48393e142129f4e7201ecf5340f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d4f0b32f3349bf938a82cf3451f955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fe170bec1d46ea8e57c14a4777db41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe2 = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:42.397609Z",
     "iopub.status.busy": "2024-04-12T19:08:42.397319Z",
     "iopub.status.idle": "2024-04-12T19:08:43.387368Z",
     "shell.execute_reply": "2024-04-12T19:08:43.386242Z",
     "shell.execute_reply.started": "2024-04-12T19:08:42.397584Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = pipe2([transcription['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:43.389600Z",
     "iopub.status.busy": "2024-04-12T19:08:43.388807Z",
     "iopub.status.idle": "2024-04-12T19:08:43.395724Z",
     "shell.execute_reply": "2024-04-12T19:08:43.394600Z",
     "shell.execute_reply.started": "2024-04-12T19:08:43.389554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"After she was thrown out, a puppy came running up. Who did she meet in the palace? A boy. Two men. They were trying to find, they were tried to audition, weren't they? People to pretend to be her. So they could get the money from the grandma.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:08:43.398465Z",
     "iopub.status.busy": "2024-04-12T19:08:43.397459Z",
     "iopub.status.idle": "2024-04-12T19:11:13.252565Z",
     "shell.execute_reply": "2024-04-12T19:11:13.251347Z",
     "shell.execute_reply.started": "2024-04-12T19:08:43.398430Z"
    }
   },
   "outputs": [],
   "source": [
    "trans2 = pipe('/kaggle/input/testing-audio/videoplayback.weba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:11:13.255339Z",
     "iopub.status.busy": "2024-04-12T19:11:13.254394Z",
     "iopub.status.idle": "2024-04-12T19:11:13.260228Z",
     "shell.execute_reply": "2024-04-12T19:11:13.259421Z",
     "shell.execute_reply.started": "2024-04-12T19:11:13.255303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Here we are again with another English test for you, this time with the movie Ratatouille. Get ready because we have 21 vocabulary questions for you to answer today. I'm sure you will... Excuse me. Hello? Ethan! Hey man, how's it going? All good, yeah, I'm just filming a lesson here for the channel. Test your English with Ratatouille, you know? Mm-hmm. Okay, so I should tell the viewers to subscribe to the channel, because every week we put out videos to help them understand their favorite movies and TV series, right? And also test their English from time to time, correct? Uh-huh. Without getting lost, yeah, without missing the jokes, and without subtitles. Got it. Will do. Thanks, man. Yeah, talk to you soon. Ethan, you know, I guess he's making sure I'm doing my job correctly here and well, So yeah, I don't have to tell you again what I just shared with him, right? I think you got the idea and the message, so please subscribe. Now let's get started with the test. Which word is closest in meaning to the adjective dazzling? Popular, expensive, impressive. Dazzling refers to something that is extremely impressive or brilliant, often in a visually captivating way. Here are some examples. A dazzling performance leaves the audience impressed and captivated. Beyonce's concerts are known for their dazzling choreography and extravagant stage effects. Similarly, a dazzling fireworks display can light up the night sky with bursts of color and spectacle, leaving people amazed. Whoa, whoa, whoa, whoa! Don't eat that! What's going on here? Turns out that funny smell was rat poison. Suddenly dad didn't think my talent was useless. What is the opposite of useless? Useful If something is useless, it has no value or practical application. On the other hand, if something is useful, it is valuable and helps to do something well. For example, this computer is useless. I need to buy a new one. Knowing how to swim is a useful life skill to have. If you are what you eat, then I only want to eat the good stuff. But to my dad... Food is fuel. If you get picky about what you put in the tank, your engine is gonna die. Now shut up and eat your garbage. If you are picky about something, you are... Careful Selective Thoughtful Peaky describes someone who is excessively selective or particular, often when it comes to choices or preferences. For example, in this context, a picky eater has very specific tastes and may refuse to eat certain foods based on texture, flavor or appearance. The word picky can also be used in other situations, to communicate this idea of being highly selective about something. In other words, it's hard to please a picky person because this person has very specific standards or preferences. Good food is like music you can taste, color you can smell, there is excellence all around you, you need only be aware to stop and savor it. Each person is savoring their wine. Savor means to fully enjoy or appreciate something, often by taking the time to enjoy its taste, aroma or other qualities. It involves experiencing something deeply and attentively, often with a sense of pleasure or satisfaction. For example, when you savor a delicious meal, you take your time to enjoy each bite, savoring the flavors and textures. Similarly, you might savor a beautiful sunset by pausing to admire its colors and appreciate the moment. Beyond just food or sensory experiences, you can also savor moments of success, love, or accomplishment by fully immersing yourself in the joy or contentment they bring. If something is not for the faint of heart, it is not challenging? Easy, special. The expression not for the faint of heart is used to describe activities, situations or experiences that are challenging, demanding or intense, and that often require courage, resilience or strong nerves to do. For example, skydiving is not for the faint of heart. It requires bravery and a willingness to confront fear. Working in emergency medicine is not for the faint of heart. It involves high-pressure situations and quick decision-making. Starting a business from scratch is not for the faint of heart. It requires perseverance and resilience to overcome obstacles. Pure poetry. But it was not to last. Augusto's restaurant lost one of its five stars after a scathing review by France's top food critic, Anton Higo. If something is scathing, it is harsh, severe, unfair. Scathing describes something that is severely critical or harsh. often in terms of language or criticism. For example, the film received scathing reviews from critics, who criticized its poor acting and shallow storyline. The columnist wrote a scathing article exposing the company's unethical business practices and lack of accountability. family all my friends probably forever to mope means to be attentive to be sad to be starving Mope refers to someone who is sad, disinterested, or lacking in energy. It can also describe the act of being in a state of melancholy, typically characterized by a lack of motivation or enthusiasm. For example, after receiving the bad news, he spent the rest of the day moping around the house, unable to muster the energy to do anything. Stop moping and try to find something positive to focus on. She couldn't overcome her moping mood after the breakup. Let me ask you something. When you watch one of our lessons here, like this one for example, do you take notes like this on paper of vocabulary that you learn with us? I used to do that a lot, you know, back in the day. You see, pages and pages of notes. But you know what? I have good news for you because you don't need to do this anymore. You know, you don't need to take lots and lots of notes to learn or remember or review the vocabulary we teach you in these lessons here. You know why? Because we have just released a new feature on the RealLife English app. the standalone flash deck cards for this very lesson. Instead of having to take a lot of notes, you can watch this lesson on the app and activate a deck of exclusive flash cards for you to practice all this vocabulary you're learning today. Isn't that cool? So join a select group of learners who use the RealLife app to expand their active vocabulary. To claim your flash cards for this lesson, just download the app and follow the instructions. I'm sure you will love it. How can we claim to represent the name of Gusteau if we don't uphold his most cherished belief? And what belief is that, Mademoiselle Tatou? Anyone can cook. To uphold a belief means to defy a belief, to support a belief, to challenge a belief. Uphold means to support, maintain, or defend a principle, belief, or value. It involves ensuring that something remains intact or respected. In this context, they're talking about upholding Gusteau's belief that anyone can cook. Perhaps I have been a bit harsh on our new garbage boy. He has taken a bold risk, and we should reward that as Chef Cousteau would have. If someone is bold, this person is brave, daring, confident. The three alternatives are correct. Bold can be defined as showing a willingness to take risks, to be daring, confident and not afraid to stand out or make a statement. For example, she made a bold move by quitting her job to start her own business. His bold decision to speak up during the meeting impressed everyone. You are a sneaky, overreaching little... RUN! If someone is called overreaching, it means This person is not working hard enough This person is trying to do more than they can manage This person is lying to gain advantage If you call someone overreaching, you're suggesting that they are trying to do too much or are extending themselves beyond reasonable limits. If somebody is being overly ambitious and trying to do more than they are expected to, they could be called overreaching. Keep in mind that this word has a negative connotation. It is used to criticize someone. For example, his overreaching attempt to manage three major projects simultaneously resulted in missed deadlines and poor quality work. Do you know what would happen to us if anyone knew we are the rats? our kitchen they'd close us down our reputation is hanging by a thread as it is if something is hanging by a thread it could start at any moment it could collapse at any moment it could get better at any moment when we say something is hanging by a thread it means that it's in a very fragile or unstable situation. If that threat breaks, everything might fall apart. In other words, failure could happen at any moment. For example, the company's financial situation was hanging by a thread when they found a rich investor to back the project. Idiot! I knew this would happen! I let him right into my place and tell him what's mine is his! Eggs gone! Stupid! He's stolen food and hit the road! What did I expect? The expression to hit the road means to rob, to leave, to cook. Hit the road is an idiomatic expression that means to leave or depart, especially on a journey or trip. It's often used informally to indicate that it's time to start traveling or to begin a journey, but you can also use it like we saw in the clip, to say that somebody has gone away. For example, I wish I could stay longer, but we need to hit the road now if we want to arrive home early. We just need to work out a system so that I do what you want in a way that doesn't look like I'm being controlled by a tiny rat. Chef, oh, would you listen to me? I'm insane, I'm insane, I'm insane, a refrigerator talking to a rat. The phrase I will never pull this off could be rephrased as I will never work in a gourmet restaurant, I will never cook again, I will never make this work. To pull something off means to succeed at doing something which seems too difficult be achieved. If you pull something off, you manage to do something well, even though the situation might not be so favorable. For example, even though she started singing only as an adult, her debut album was a hit success. I can't believe she was able to pull it off. But still I'm here? How did this happen? Because, well, because you, uh... Because I'm the toughest cook in this kitchen. I worked too hard for too long to get here, and I'm not going to jeopardize it for some garbage boy who got lucky. Got it? If you jeopardize something, you make it happen, you put it in danger, you take care of it. Jeopardize means to put something at risk or in danger of being harmed or lost. It's like when you do something that could make a situation worse or cause a problem. For example, leaving your homework until the last minute might jeopardize your chances of getting a good grade. The deadline in the will expires in less than a month! Suddenly some boy arrives with a letter from his recently deceased mother claiming gusto as his father. Deceased means declared, alive, dead. Deceased is an adjective used to describe someone who has died. It is often used in formal or official contexts to refer to someone who has passed away. For example, the deceased person's family gathered to mourn their loss at the funeral. Then what are you worried about? If he works here, you'll be able to keep an eye on him while I do a little digging. Find out how much of this is real. I will need you to collect some DNA samples from the boy. The phrase to do a little digging means to investigate, to test, to locate. To do a little digging or to do some digging is an idiom that means to conduct thorough or complete research or investigation into a particular topic or issue, often involving gathering information or uncovering facts that are not so evident. For example, I need to do some digging to find out more about the company's financial history before making a decision to invest. Mark my words! The whole thing is highly suspect! He knows something. The phrase mark my words could be rephrased as Don't listen to me at all. I really believe in what I'm saying. Remember what I am about to tell you. You can say mark my words when you want the other person to remember something you're about to say. By saying this, you show that what you're about to say is important and relevant, and the other person should listen closely. For example, mark my words. If you don't work on your communication skills, you won't get very far professionally. No one knows for sure. He changes the story every time you ask him. I defrauded a major corporation. I robbed the second largest bank in France using only a ballpoint pen. A similar word to defraud is to mislead, to close, to start a business. To defraud means to illegally obtain money or property from someone by deceiving them or using fraudulent methods. It involves intentionally misleading or tricking someone for personal gain. For example, he defrauded investors out of millions of dollars by promising high returns on fake investment opportunities. What do I tell them? What did you tell them? I told them I would ask! What are you blathering about? Customers are asking what is new. What should I tell them? To blather about something means to stop talking about something, to talk continuously about something, to talk about something carefully. To blather about is an informal expression that means to talk continuously and often without much substance. If someone blathers about something, this person speaks in a rambling, foolish or meaningless manner. For example, during the meeting, he just blathered about unrelated topics, wasting everyone's time. You look thin. Why is that? A shortage of food or a surplus of snobbery? A shortage of something is a small quantity, a large quantity. If you say there is a shortage of something, you mean there is too little of something or there isn't enough of something. This is a more advanced way to say this. For example, some people believe there is a shortage of jobs, but I believe there is a shortage of qualified people. A surplus of something is... a small quantity, a large quantity. Nice, right? A surplus of something is the opposite of a shortage of something. When there is a surplus of something, there is too much of it, more than you need or want. For example, the dinner last night was great, but now there is a surplus of food and I don't know what to do with it. Alright, time for the results. If you got between 6 and 12 questions right, your vocabulary level could be around B2, or upper-intermediate. Now, if you got between 7 and 14 questions right, your vocabulary level could be around C1, or advanced. Finally, if you got between 15 and 21 questions right, your vocabulary level could be around C2, proficient. A couple of things to keep in mind, though. If you got most questions correctly but still feel like you need to improve your vocabulary, this might be because these words are already part of your passive vocabulary but not part of your active vocabulary. So, work on actively using these words when you speak or write in English. Multiple choice questions tend to be easier to answer because you can answer correctly by elimination. Having the context in which the word is being used with the scenes makes it easier for you to deduce the meaning of some words, simply by observing the scene. And finally, this is an informal test and our main purpose here is to teach you some more advanced words while having fun. This does not replace a proper proficiency exam. In order to assess your English level effectively, aside from vocabulary, you will need to be tested on other skills, such as speaking, writing, listening, and grammar. But now, let me give you the challenge for today. Pick three words from today's test and create sentences with them. Post your sentences in the comment section below because I'm curious to see them. Guys, great job! You've made it until the end of the lesson, or test. You rock! Don't forget to like this video, share this video with a friend who's also learning English, and of course, subscribe if you're not a subscriber yet. And if you wanna keep learning English while having a ton of fun, check out this lesson next. I'll be talking to you soon. Bye bye. A foe is an adversary or an enemy, someone who opposes or is in conflict with another person or group. For example, every superhero has a formidable foe. Batman has the Joker, Superman has Lex Luthor, and Spider-Man has Venom.\"}\n"
     ]
    }
   ],
   "source": [
    "print(trans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:11:13.262208Z",
     "iopub.status.busy": "2024-04-12T19:11:13.261784Z",
     "iopub.status.idle": "2024-04-12T19:11:35.603163Z",
     "shell.execute_reply": "2024-04-12T19:11:35.602156Z",
     "shell.execute_reply.started": "2024-04-12T19:11:13.262163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_34/2001622090.py\", line 10, in <module>\n",
      "    summary = pipe2(chunk,max_length=60)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py\", line 269, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text2text_generation.py\", line 167, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "source": [
    "input_text = trans2['text']\n",
    "\n",
    "# Split the input text into smaller chunks\n",
    "max_chunk_length = 512  # Define the maximum length for each chunk\n",
    "chunks = [input_text[i:i + max_chunk_length] for i in range(0, len(input_text), max_chunk_length)]\n",
    "\n",
    "# Summarize each chunk individually\n",
    "summaries = []\n",
    "for chunk in chunks:\n",
    "    summary = pipe2(chunk,max_length=60)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Combine the summaries if needed\n",
    "# combined_summary = \" \".join(summaries)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:11:35.605157Z",
     "iopub.status.busy": "2024-04-12T19:11:35.604493Z",
     "iopub.status.idle": "2024-04-12T19:11:35.610526Z",
     "shell.execute_reply": "2024-04-12T19:11:35.609579Z",
     "shell.execute_reply.started": "2024-04-12T19:11:35.605122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethan: \"Get ready because we have 21 vocabulary questions for you to answer today\" Ethan: \"Test your English with Ratatouille, you know? Mm-hmm. Okay, so I should tell the viewers to subscribe to the channel, because every week we\n",
      "Ethan: I guess he's making sure I'm doing my job correctly here and well, So yeah, I don't have to tell you again what I just shared with him, right? I think you got the idea and the message, so please subscribe. Thanks, man.\n",
      "Dazzling refers to something that is extremely impressive or brilliant, often in a visually captivating way. Beyonce's concerts are known for their dazzling choreography and extravagant stage effects. A dazzling fireworks display can light up the night sky with bursts of color and spectacle, leaving people amazed\n",
      "If something is useless, it has no value or practical application. On the other hand, if something is useful, it is valuable and helps to do something well. Knowing how to swim is a useful life skill to have. If you are what you eat, then I only want to\n",
      "Careful Selective Thoughtful Peaky describes someone who is excessively selective or particular. A picky eater has very specific tastes and may refuse to eat certain foods based on texture, flavor or appearance. If you get picky about what you put in the tank, your engine is gonna\n",
      "Savor means to fully enjoy or appreciate something, often by taking the time to enjoy its taste, aroma or other qualities. Good food is like music you can taste, color you can smell, there is excellence all around you, you need only be aware to stop and savor it\n",
      "You can savor moments of success, love, or accomplishment by fully immersing yourself in the joy or contentment they bring. savor a beautiful sunset by pausing to admire its colors and appreciate the moment. eply and attentively, often with a sense of pleasure or\n",
      "The expression not for the faint of heart is used to describe activities, situations or experiences that are challenging, demanding or intense. For example, skydiving requires bravery and a willingness to confront fear. Working in emergency medicine involves high-pressure situations and quick decision-making. Starting a\n",
      "Augusto's restaurant lost one of its five stars after a scathing review by France's top food critic, Anton Higo. If something is scathing, it is harsh, severe, unfair. Scathing describes something that is severely critical or harsh. often in terms of language or criticism.\n",
      "Mope refers to someone who is sad, disinterested, or lacking in energy. It can also describe the act of being in a state of melancholy, typically characterized by a lack of motivation or enthusiasm. For example, after receiving the bad news, he spent the rest of the day\n",
      "Stop moping and try to find something positive to focus on. When you watch one of our lessons, like this one for example, do you take notes like this on paper of vocabulary that you learn with us? I used to do that a lot, you know, back in the\n",
      "The RealLife English app now has a standalone flash deck cards for this very lesson. Instead of having to take a lot of notes, you can watch this lesson on the app and activate a deck of exclusive flash cards for you to practice all this vocabulary you're learning today.\n",
      "Uphold means to support, maintain, or defend a principle, belief, or value. How can we claim to represent the name of Gusteau if we don't uphold his most cherished belief? And what belief is that, Mademoiselle Tatou? Anyone can cook\n",
      "Bold can be defined as showing a willingness to take risks, to be daring, confident and not afraid to stand out or make a statemen. volves ensuring that something remains intact or respected. In this context, they're talking about upholding Gusteau's belief that anyone can\n",
      "If you call someone overreaching, you're suggesting that they are trying to do too much. For example, she made a bold move by quitting her job to start her own business. His bold decision to speak up during the meeting impressed everyone. You are a sneaky, overreaching little\n",
      "Overreaching has a negative connotation. It is used to criticize someone. For example, his overreaching attempt to manage three major projects simultaneously resulted in missed deadlines and poor quality work. Do you know what would happen to us if anyone knew we are the rats? our kitchen they'd\n",
      "When we say something is hanging by a thread it means that it's in a very fragile or unstable situation. If that threat breaks, everything might fall apart. In other words, failure could happen at any moment. For example, the company's financial situation was hanging by the thread when\n",
      "Hit the road is an idiomatic expression that means to leave or depart, especially on a journey or trip. It's often used informally to indicate that it's time to start traveling or to begin a journey. You can also use it like we saw in the clip, to say\n",
      "\"We just need to work out a system so that I do what you want in a way that doesn't look like I'm being controlled by a tiny rat,\" he says. \"To pull something off means to succeed at doing something which seems t. o hit the road now if\n",
      "\"I worked too hard for too long to get here, and I'm not going to jeopardize it for some garbage boy,\" she says. \"Because I'm the toughest cook in this kitchen,\" she adds. \"I can't believe she was able to pull it off\"\n",
      "If you jeopardize something, you make it happen, you put it in danger, you take care of it. Jeopardize means to put something at risk or in danger of being harmed or lost. For example, leaving your homework until the last minute might jeopardize your chances of\n",
      "Deceased means declared, alive, dead. It is often used in formal or official contexts to refer to someone who has passed away. If he works here, you'll be able to keep an eye on him while I do a little digging. Find out how much of this is\n",
      "To do a little digging is an idiom that means to conduct thorough or complete research or investigation into a particular topic or issue. For example, I need to do some digging to find out more about the company's financial history before making a decision to invest. The phrase to do a\n",
      "The phrase mark my words could be rephrased as Don't listen to me at all. By saying this, you show that what you're about to say is important and relevant. If you don't work on your communication skills, you won't get very far professionally.\n",
      "A similar word to defraud is to mislead, to close, to start a business. To defraud means to illegally obtain money or property from someone by deceiving them or using fraudulent methods. It involves intentionally misleading or tricking someone for personal gain. For example, he defra\n",
      "To blather about is an informal expression that means to talk continuously and often without much substance. If someone blathers about something, this person speaks in a ramb. What are you blathering about? Customers are asking what is new. What should I tell them?\n",
      "A shortage of something is a small quantity, a large quantity. For example, some people believe there is a shortage of jobs, but I believe there isn't. This is a more advanced way to say this. If you say there is too little of something, you mean there is\n",
      "A surplus of something is the opposite of a shortage of something. When there is too much of something, there is more than you need or want. For example, the dinner last night was great, but now there is a surplus of food and I don't know what to do with\n",
      "If you got 7 to 14 questions right, your vocabulary level could be around C1, or advanced. If you got between 15 and 21 questions right your vocabulary could be about C2, proficient. A couple of things to keep in mind, though, if you got most questions correctly\n",
      "Multiple choice questions tend to be easier to answer because you can answer correctly by elimination. Having the context in which the word is being used with the scenes makes it easier for you to deduce the meaning of some words. And finally, this is an informal test and our main purpose here\n",
      "Pick three words from today's test and create sentences with them. Post your sentences in the comment section below because I'm curious to see them. Don't forget to like this video, share this video with a friend who's also learning E. ur English level effectively.\n",
      "A foe is an adversary or an enemy, someone who opposes or is in conflict with another person or group. For example, every superhero has a formidable foe. Check out this lesson next to learn English while having a ton of fun. I'll be talking to you soon. Bye bye\n"
     ]
    }
   ],
   "source": [
    "for i in summaries:\n",
    "    print(i[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:50:08.033079Z",
     "iopub.status.busy": "2024-04-12T19:50:08.032337Z",
     "iopub.status.idle": "2024-04-12T19:52:00.502330Z",
     "shell.execute_reply": "2024-04-12T19:52:00.501396Z",
     "shell.execute_reply.started": "2024-04-12T19:50:08.033045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Speech. It's the most natural form of human communication. This is my demo of a real-time speech recognition system using deep learning. Yo, what's up world? Michael here, and today we're going to be talking about speech recognition, why it's hard, and how deep learning can help solve it. Later in this video, we're going to build our own neural network and train the speech recognition model from scratch. So humans are really good at understanding speech, so you would also think it's easy for computers to do too as well, right? Speech recognition is actually really hard for computers. Speech is essentially sound waves, which lives in the physical world with their own physical properties. For example, a person's age, gender, style, personality, accent, all affects how they speak and the physical properties of sound. A computer also got to consider the environmental noise around the speaker and the type of microphones they're using to record. So because there's so many variations and nuances in the physical properties of speech, it makes it extremely hard to come up with all the rules possible for speech recognition. Not only do you have to deal with the physical properties of speech, but you have to deal with the linguistic properties of it as well. Consider this sentence. I read a book last night. I like to read. read and read are spelled the same but they sound different red can be spelled like the color red but in this context it needs to be spelled like red for reading so language itself is really complex and it has a lot of nuance and variations that you would have to come up with all possible rules for it as well to have an effective speech recognition so what do you do when you have like what seems like an insurmountable amount of rules you use deep learning so deep learning has changed the game for a lot of complex tasks. Any modern speech recognition system today leverages deep learning in some way. So to build an effective speech recognition system, we have to have a strategy on how to tackle the physical properties of speech as well as the linguistic properties of it. Let's start with the physical property. To properly deal with the variations and nuances that comes with the physicality of speech like age, gender, microphone, environmental condition, etc., we'll build an acoustic model. On a high level, our acoustic model will be a neural network that takes in speech waves as input and outputs to transcribe text. In order for our neural network to know how to properly transcribe the speech waves to texts, we'll have to train it with a ton of speech data. Let's first consider what model we want to use by looking at the type of problem we are trying to solve. Speech is a naturally occurring time sequence meaning we need a narrow network that can process sequential data. The narrow network also needs to be lightweight in terms of memory and compute because we want to run it real time on everyday consumer machines. Recurrent neural networks, or RNN for short, are a natural fit for this task as it excels at processing sequential data even when we configure it to be a smaller network size. So we'll use that as our acoustic model. Now let's consider the data. What a neural network can learn is dependent on the data you train it with. If we want our neural network to learn the nuances of speech, we'll need speech data that has a lot of variations of gender, age, accent, environmental noise, and types of microphones. Common Voice is an open-source speech dataset initiative led by Mozilla that has many of those variations, so it'll be perfect. Let's listen to a couple samples. Gordon no longer mentions bootlaces, and the two engines soon talk about trucks. Favors granted Jews, Latin colonies continue. It has since been renamed Yerza Araft International Airport. Each audio sample comes with labels of the transcribed text. Okay, I think we have a solid approach. We have a dataset that contains some variation in nuances as well as a lightweight neural network architecture. Now, let's talk about the linguistic aspect of speech. To inject linguistic features into the transcriptions, we'll use something called a language model alongside a rescoring algorithm. To understand how this works at a high level, let's look at the acoustic model's output. A speech recognition neural network is probabilistic, so for each time step it outputs a probability of possible words. You can naively take the highest probable word of each time step and emit that as your transcript, but your network can easily make linguistic mistakes like using the word red for color when it should be red for reading. This is when the language model comes into play. A language model can determine what is a more likely sentence by building a probability distribution over sequences of words it trains on. You can use a language model and rescore the probabilities depending on the context of the sentence. You'll get an idea of how all this works in a minute. For our implementation of a language model, we can use an open source project called KenLM, which is a rules-based language model. We want to use KenLM because it's lightweight and super fast, unlike the much heavier neural network-based language models. A narrow-based language model like transformers from Hugging Face have been proven to produce better results, but since our goal is low compute, we'll use KenLM which works well enough. For the rescoring algorithm, we'll use what's called a CTC beam search. The beam search combined with the language model is how we'll rescore the outputs for better transcriptions. The logic of the beam search algorithm can get pretty complex and very boring and I'm way too busy to attempt an in-depth explanation here, but here's the basic premise. The beam search algorithm will traverse the outputs of the acoustic model and use the language model to build hypotheses aka beams for the final output. During the beam search, if the language model sees that the word book exists in the transcription, it will boost the probability of the beams that contains red like reading instead of red like color because it makes more sense. This process will produce more accurate transcriptions. Thank the programming guides for open source code so we don't have to build the language model and the CTC beam search rescoring algorithm ourselves, which will save us a ton of time. So to sum it up, using a language model and a CTC beam search algorithm, we can inject language information into the acoustic model's output, which results in more accurate transcriptions. Okay, I think we have a pretty solid strategy on how to tackle this speech recognition problem. For the physical properties, we'll implement an acoustic model. For the linguistic properties, we'll implement a language model with a rescoring algorithm. Let's get to building. First we need to build a data processing pipeline. We'll need to transform the raw audio waves into what's called MEL specialgrams. You can just think of MEL specialgrams as a pictorial representation of sound. We'll also need to process our character labels. Our models will be character based, meaning it will output characters instead of a word probabilities. Decoding character probabilities is more efficient because we only have to worry about 27 probabilities for each output instead of like a hundred or thousand of possible words. We need to augment our data so we can effectively have a bigger data set. We use spec augment. Spec augment is an augmentation technique that cuts out the information in the time and frequency domain, effectively destroying pieces of the data. This makes the neural network more robust because it's forced to learn how to make correct predictions with imperfect data, making it more generalizable to real world. Now let's move on to the model. The model consists of a convolutional layer, three dense layer, and an R and an LSTM layer. The purpose of the convolutional layer is two things. It learns to extract better features from the MEL spectrogram and also reduce the time dimensions of the data. In theory, the CNN layer will produce features that should be more robust causing the RNNs to produce better predictions. We also set the stride of the CNN layer to 2, therefore reducing the time steps of the MEL spectrogram by half, allowing the RNNs to do less work because there are less time steps which would make the overall network faster. We add in two more dense layers in between the CNN and the RNNs. The purpose of the dense layer is to also learn to produce a more robust set of features for the RNNs. For the RNN layer, we used an LSTM variant. The RNN takes the features produced by the previous dense layer and step-by-step produces an output that can be used for prediction. We also have a final dense layer with a softmax activation that acts like a classifier. The classifier takes the RNN's output and predicts character probabilities for each time step. We add layer normalization, GILU activation, and dropout between each layer with the purpose of making the network more generalizable and robust to real-world data. In deep learning, adding more layers can lead to better results, but since we want this to be a lightweight model, we stop at five layers. We have one CNN layer, one LSDM RNN layer, and three dense layers. For the training script, we'll use PyTorch Lightning. PyTorch Lightning is a library for PyTorch that decouples the science code from the engineering code. For the training objective, I use CTCLost function, which makes it super easy to train speech recognition models. It can assign probabilities given an input, making it possible to just have your audio sample paired with their corresponding text labels without needing to align each character to every single frame of the audio. Oh, hey, just finishing up the rest of the code here. This code is open source, so if you want to see the full implementation details, make sure you check out the GitHub repo in the description below. Now that we have the code, we need to start training. This is the perfect time to introduce my training rig, War Machine. War Machine is my personal deep learning rig I use to train models. If you're following along, I recommend using a GPU. So if you don't have a GPU, you can use free alternatives like Google Collab or Kaggle Kernels. OK, let's get to training. Thank you. Alright, so training's finally finished. It took a couple of days, but I'm pretty happy with the results. Check it out. The loss curves, they both look pretty good. doesn't seem like anything's overfitting. Also while everything was training I implemented the language model and the rescoring algorithm from the open source packages. Also I made a little web demo using flask to demo the speech recognition model. So let me set everything up and let's test this thing out. Okay so I got the demo prepared. The first demo is gonna be just the acoustic model without the language model and a rescoring algorithm. This is to showcase showcase why it's important. Hi, this is Michael, demoing my speech recognition system without a language model and a rescoring algorithm. As you can see, it's shit. It's not very good. Let me set up the second demo. Okay, so this is the second demo with the language model and the rescoring algorithm. Hi, this is Michael demoing my speech recognition system with a language model and a rescoring algorithm. As you can see it does a lot better, but it's not perfect. Okay so the speech recognition system with the language model and the rescoring algorithm works pretty well with me. But let me reveal something I haven't mentioned yet. I collected about an hour of my speech and trained it with the common voice dataset which is about 1000 hours. One thing I forgot to mention when filming this clip was that I also upsampled the 1 recording of myself to about 50 hours so it can be more representative in the entire training data set. Okay, play. So there is a possibility that that extra hour of my voice has biased the algorithm to work really well with me. So to test that theory, I want to try the speech recognition system with other people. Yeah, this is my brother long long. I need you to test this out to me real quick Alright, just press that start button and say whatever you want Hi, baby girl Say something else you look fine Okay, say some fourth tone Hey, you better quit around Okay, as you can see it doesn't work very well on his voice, but when I start talking you can see it start picking up on what I'm saying, right? Oh dang, that's cool. Okay, thank you. Okay, so our next guest is big on privacy, so we'll just skip the introductions and go straight into the demo. And play. Say anything you want. I love Charlie, Baby, and Oliver. Say something else. This thing sucks. Okay, as you can tell, it only works well for me again. It doesn't work very well for her, so she thinks it sucks. Alright so as you can tell from my guest's reaction, the speech recognition system is not that great. At least on them. Works really well for me. That's because I biased the algorithm by adding my own data. All of this was expected though. Deep Speech 2, a very famous speech recognition paper from Baidu, claimed that you need about 11,000 hours of audio data to have an effective speech recognition system, and we used like what a thousand hours or so? Their model also has 70 million parameters compared to our model which is 4 million parameters. I chose a small architecture on purpose because I want it to be small and I want it to run real time on any consumer device like my laptop. So I think overall the system works pretty well if you can collect your own data. So if you want to train your own speech recognition system, I recommend you collect your own data using something like the Mimic Recording Studio which is what I used. I'll also open source a pre-trained model that you can download and then you can just fine tune that on your own data so you don't have to go through the trouble of training it on the 1000 hours of common voice like I did. I have the links to all of the goodies in the description, so make sure you check that out. So this video was part of the series of how to build your own AI voice assistant using PyTorch. So far, I've done the wake word detection, which is the first video. Now I did the automatic speech recognition. I still have the natural language understanding part, which is the way to map the transcription to some sort of action, like what's the weather like. And then I also have to do the speech synthesis part, which is the synthetic voice of the AI voice assistant. So if you want to be updated when those videos come out, make sure you hit that like and subscribe button. Also, I have a Discord server that's getting pretty active. If you want a community of AI enthusiasts, practitioners, and hackers, make sure you join the Discord server. We want to start planning events in the Discord server, so if you're curious about what they are, then make sure you join. Okay, so that's it for this video. And as always, thanks for watching. Thank you.\"}\n"
     ]
    }
   ],
   "source": [
    "trans3 = pipe('/kaggle/input/testing-audio/Y2meta.app - I Built a Personal Speech Recognition System for my AI Assistant (128 kbps).mp3')\n",
    "print(trans3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-12T19:52:00.504716Z",
     "iopub.status.busy": "2024-04-12T19:52:00.504284Z",
     "iopub.status.idle": "2024-04-12T19:52:19.078981Z",
     "shell.execute_reply": "2024-04-12T19:52:19.078226Z",
     "shell.execute_reply.started": "2024-04-12T19:52:00.504680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 60, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n",
      "Your max_length is set to 60, but your input_length is only 3. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=1)\n"
     ]
    }
   ],
   "source": [
    "input_text2 = trans3['text']\n",
    "\n",
    "# Split the input text into smaller chunks\n",
    "max_chunk_length2 = 512  # Define the maximum length for each chunk\n",
    "chunks23 = [input_text2[i:i + max_chunk_length2] for i in range(0, len(input_text), max_chunk_length2)]\n",
    "\n",
    "# Summarize each chunk individually\n",
    "summaries2 = []\n",
    "for chunk in chunks23:\n",
    "    summary = pipe2(chunk,max_length=60,min_length=5)\n",
    "    summaries2.append(summary)\n",
    "\n",
    "# Combine the summaries if needed\n",
    "# combined_summary = \" \".join(summaries)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4775284,
     "sourceId": 8088944,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4780935,
     "sourceId": 8102930,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
